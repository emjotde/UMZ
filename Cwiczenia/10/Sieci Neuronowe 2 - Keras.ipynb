{
 "metadata": {
  "name": "",
  "signature": "sha256:516638cb061d58c2fa29fcf2176c62654bcc8bc65b7aae68cf46aef18bdcec30"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Wielowarstwowe sieci neuronowe\n",
      "## Uczenie za pomoc\u0105 gotowych pakiet\u00f3w\n",
      "\n",
      "B\u0119dziemy dzi\u015b korzysta\u0107 z zaawansowanego pakietu do sieci neuronowych \"Keras\". \n",
      "\n",
      "### Instalacja\n",
      "* Pod Windowsem powinno by\u0107 wszystko zainstalowane.\n",
      "* Pod Linuxem mo\u017cna zainstalowa\u0107 lokalnie:\n",
      "        pip install --user keras\n",
      "* Lub na w\u0142asnym komputerze:\n",
      "        sudo pip install keras\n",
      "* Mo\u017cliwe, \u017ce konieczne jest ponowne uruchomienie IPython, je\u015bli by\u0142 uruchomiony podczas instalacji.\n",
      "\n",
      "### Dokumentacja\n",
      "* Dosy\u0107 pe\u0142na i obszerna dokumentacja na: http://keras.io\n",
      "* Pomocne przyk\u0142ady na: https://github.com/fchollet/keras/tree/master/examples (zaawansowane)\n",
      "* Szczeg\u00f3lnie przyk\u0142ad: mnist_mlp.py\n",
      "* ***Uwaga:*** pierwsze uruchomienie zazwyczaj trwa jaki\u015b czas, poniewa\u017c pod spodem model kompiluje si\u0119 jako aplikacja w C++. \n",
      "\n",
      "### Zadania\n",
      "\n",
      "#### MNIST\n",
      "1. Uruchom przyk\u0142ad mnist_mlp.py, warto ew. zmieni\u0107 liczb\u0119 epok do 5 \n",
      "1. Przeanalizuj kod za pomoc\u0105 dokumentacji oraz innych \u017ar\u00f3de\u0142 w internecie i opisz w notebooku:\n",
      "    1. Do jakiej postaci autorzy przyk\u0142adu sprowadzaj\u0105 dane Y_train i Y_test?\n",
      "    1. Przedstaw wz\u00f3r matematyczny na funkcj\u0119 b\u0142\u0119du (pojawi\u0142a si\u0119\u00a0na wyk\u0142adzie)\n",
      "    1. Ile warstw ma przyk\u0142adowy model, jakie s\u0105\u00a0rozmiary macierzy odpowiednich wag. Czy mo\u017cna uzyska\u0107 dost\u0119p do tych wag?\n",
      "    1. Jakie funkcje aktywacji u\u017cyto? Podaj ich wzory, dla RELU stw\u00f3rz wykres.\n",
      "    1. Co to jest Dropout? Czemu s\u0142u\u017cy? Jakie znaczenie ma parametr?\n",
      "1. Zmodyfikuj model:\n",
      "    1. Usu\u0144 warstwy Dropout, jaki jest efekt?\n",
      "    1. Stw\u00f3rz model odpowiadaj\u0105cy 10-klasowej regresji logistycznej (om\u00f3wiony na wyk\u0142adzie): jaka jest jako\u015b\u0107?\n",
      "    1. Stw\u00f3rz 6-cio warstwowy model o rozmiarach warstw 2500, 2000, 1500, 1000, 500 oraz 10 bez Dropout, u\u017cyj wsz\u0119dzie funkcji aktywacji `tanh` z wyj\u0105tkiem ostatniej warstwy, gdzie nale\u017cy u\u017cy\u0107 `softmax`. Trenuj model przez 10 epok.\n",
      "    1. Dodaj warstwy Dropout, porownaj jako\u015b\u0107 po 10 epokach. \n",
      "    1. Zamiast RMSprop u\u017cyj algorytm Adagrad, por\u00f3wnaj jako\u015b\u0107. \n",
      "\n",
      "#### XOR\n",
      "1. Poka\u017c, \u017ce jednowarstowa sie\u0107 (z jednym neuronem z sigmoidaln\u0105\u00a0funkcj\u0105 aktywacji) nie potrafi nauczy\u0107\u00a0si\u0119\u00a0funkcji XOR. Wykorzystaj funkcj\u0119 model.predict(...). Jaka funkcja bl\u0119du wydaj\u0119\u00a0sie w\u0142asciwa?\n",
      "2. Poka\u017c, \u017ce dwuwarstowy perceptron potrafi. Wykorzystuj\u0105c model typu `Graph`, zbuduj nast\u0119puj\u0105c\u0105 sie\u0107:\n",
      "        x1 -> nh\n",
      "        x2 -> nh\n",
      "        x1 -> no\n",
      "        x2 -> no\n",
      "        nh -> no\n",
      "Gdzie `x1` i `x2` to wejscia, `nh` to neuron ukryty, `no` to neuron wyj\u015bciowy. Zwr\u00f3c uwag\u0119, \u017ce po\u0142\u0105czenia wychodz\u0105 poza warstwy. W\u0142a\u015bciwym trybem \u0142\u0105czenia wej\u015b\u0107 do neuronu wyj\u015bciowego to `merge_mode='concat'` (skleja wyj\u015bcia poprzednich warstw w jeden ci\u0105g). \n",
      "3. **Uwaga**: Konieczna jest du\u017ca liczba epok, ok. 10000. Zbior trenuj\u0105cy to tylko 4 przy\u0142ady, zatem to p\u00f3jdzie szybko. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}