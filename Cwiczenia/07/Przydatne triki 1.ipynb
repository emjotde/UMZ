{
 "metadata": {
  "name": "",
  "signature": "sha256:c3187056701e756d2aef825b6c3c2e0ac407c1bd38297244990a76f23af7bf1a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Zadania\n",
      "\n",
      "**AdaGrad**\n",
      "1. Rozszerz podany poni\u017cej algorytm SGD o funkcjonalno\u015b\u0107 algorytmu\u00a0AdaGrad (w\u0142\u0105czana parametrem `adaGrad=True`). Zasady dzia\u0142ania AdaGrad s\u0105 podane w materia\u0142ach do wyk\u0142adu.<br/> **Uwaga**: podczas dzielenia przez pierwastek sumy kwadrat\u00f3w historycznych gradient\u00f3w warto doda\u0107 ma\u0142\u0105\u00a0warto\u015b\u0107 $\\epsilon=10^{-7}$ do mianownika (\u017ceby nie dzieli\u0107\u00a0przez 0).\n",
      "1. Wykorzystuj\u0105c dane z ostatnich \u0107wicze\u0144 oraz algorytm AdaGrad, stw\u00f3rz model wieloklasowej regresji logistycznej (dla wszystkich 10) z parametrami:\n",
      "    * Rozmiar wsadu: 50\n",
      "    * Liczba epok: 2\n",
      "    * Rozmiar kroku $\\alpha$: 1.0 (gdy u\u017cywamy AdaGrad $\\alpha$ mo\u017ce by\u0107 niezmienne)\n",
      "* Oblicz jego jako\u015b\u0107\u00a0na zbiorze testowym. Nast\u0119pnie sprobuj uzyska\u0107 wynik podobny lub lepszy samym algorytmem SGD bez opcji AdaGrad, modyfikuj\u0105\u0107 paramtr $\\alpha$. Da si\u0119?\n",
      "\n",
      "**Walidacja Krzy\u017cowa**\n",
      "1. Na podstawie pseudo-kodu z wyk\u0142adu zaimplementuj 5-krotn\u0105 walidacj\u0119 krzy\u017cow\u0105 na danych ucz\u0105cych. Wykorzystaj \u015bredni\u0105\u00a0poprawno\u015b\u0107\u00a0klasyfikacyjn\u0105 z 5-ciu zbior\u00f3w walidacyjnych jako ocen\u0119\u00a0jako\u015bci na ca\u0142ym zbiorze ucz\u0105cym.\n",
      "2. Dobierz na podstawie walidacji krzy\u017cowej najlepsze parametry modelu (rozmiar wsadu, liczba epok, u\u017cycie SGD z/bez AdaGrad, paramtr $\\alpha$), **nie (!)** patrz\u0105\u0107 na zbi\u00f3r testowy.\n",
      "3. Sprawd\u017a, czy tak dobrany model, po przetrenowaniu na ca\u0142ych danych jest r\u00f3wnie\u017c najlepszy na zbiorze testowym. (Nie ma niestety takiej gwarancji.)\n",
      "\n",
      "**Ensemble** (za dodatkowe 20 punkt\u00f3w)\n",
      "1. Wytrenuj 10 klasyfikator\u00f3w z powy\u017cej dobranymi parametrami tasuja\u0107 dane trenuj\u0105ce przed ka\u017cdym trenowaniem (pami\u0119taj by tasowa\u0107 odpowiedzi w tej samej kolejno\u015bci!). \n",
      "1. Oblicz predykcje (klasy) dla ka\u017cdego z nich na zbiorze testowym oraz jako\u015b\u0107 ka\u017cdego\u00a0klasyfikatora.\n",
      "1. Dla ka\u017cdego przyk\u0142ady wybierz klas\u0119, kt\u00f3ra wyst\u0119powa\u0142a najcz\u0119\u015b\u0107iej w odpowiedziach 10 klasyfikator\u00f3w jako now\u0105 klas\u0119. \n",
      "1. Oblicz jako\u015b\u0107\u00a0tak uzyskanych predykcji i por\u00f3wnaj z jako\u015bci\u0105 pojedynczych klasyfikator\u00f3w. Pomog\u0142o?\n",
      "\n",
      "<br/>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def runningMeanFast(x, N):\n",
      "    return np.convolve(x, np.ones((N,))/N, mode='valid')\n",
      "\n",
      "def safeSigmoid(x, eps=0):\n",
      "    y = 1.0/(1.0 + np.exp(-x))\n",
      "    # przytnij od dolu i gory\n",
      "    if eps > 0:\n",
      "        y[y < eps] = eps\n",
      "        y[y > 1 - eps] = 1 - eps\n",
      "    return y\n",
      "\n",
      "def h(theta, X, eps=0.0):\n",
      "    return safeSigmoid(X*theta, eps)\n",
      "\n",
      "def J(h,theta,X,y):\n",
      "    m = len(y)\n",
      "    f = h(theta, X, eps=10**-7)\n",
      "    return -np.sum(np.multiply(y, np.log(f)) + \n",
      "                   np.multiply(1 - y, np.log(1 - f)), axis=0)/m\n",
      "\n",
      "def dJ(h,theta,X,y):\n",
      "    return 1.0/len(y)*(X.T*(h(theta,X)-y))\n",
      "\n",
      "def softmax(X):\n",
      "    return np.exp(X)/np.sum(np.exp(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def SGD(h, fJ, fdJ, theta, X, y, \n",
      "        alpha=0.001, maxEpochs=1.0, batchSize=100):\n",
      "    m, n = X.shape\n",
      "    start, end = 0, batchSize\n",
      "    \n",
      "    maxSteps = (m * float(maxEpochs)) / batchSize\n",
      "    for i in range(int(maxSteps)):\n",
      "        XBatch, yBatch =  X[start:end,:], y[start:end,:]\n",
      "\n",
      "        theta = theta - alpha * fdJ(h, theta, XBatch, yBatch)\n",
      "        \n",
      "        if start + batchSize < m:\n",
      "            start += batchSize\n",
      "        else:\n",
      "            start = 0\n",
      "        end = min(start + batchSize, m)\n",
      "        \n",
      "    return theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}