{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:a48f81e53ae71f120d6b8a840d63a589ed6ea0a9e638d2b174da214dd659af06"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Klasyfikacja za pomoc\u0105 sieci neuronowych \n",
      "\n",
      "## Przypomnienie materia\u0142u z wyk\u0142adu\n",
      "\n",
      "Podczas klasyfikowania danych wej\u015bciowych za pomoc\u0105 prostych jednokierunkowych sieci neuronowych (*ang.* **feedforward**) obliczamy warto\u015bci neuron\u00f3w na podstawie warto\u015bci neuron\u00f3w poprzedniej warstwy.\n",
      "\n",
      "* Maj\u0105c dan\u0105 $n$-warstwow\u0105 sie\u0107 neuronow\u0105 oraz jej parametry $\\Theta^{(1)}, \\ldots, \\Theta^{(n)} $ oraz $\\beta^{(1)}, \\ldots, \\beta^{(n)} $ liczymy: \n",
      "$$a^{(i)} = g^{(i)}\\left( a^{(i-1)} \\Theta^{(i)} + \\beta^{(i)} \\right). $$ \n",
      "* Funkcje $g^{(i)}$ to tzw. **funkcje aktywacji**.\n",
      "* Dla $i = 0$ przyjmujemy $a^{(0)} = \\mathrm{x}$ (wektor wierszowy cech) oraz $g^{(0)}(x) = x$ (identyczno\u015b\u0107).\n",
      "* W przypadku klasyfikacji, cz\u0119sto dla ostatniej warstwy $n$ (o rozmiarze r\u00f3wnym liczbie klas) przyjmuje si\u0119 $g^{(n)}(x) = \\mathop{\\mathrm{softmax}}(x)$.\n",
      "* Wtedy funkcja decyzyjna ma posta\u0107:\n",
      "$$c = \\mathop{\\mathrm{argmax}}_{i}\\left(a^{(n)}_i\\right)$$\n",
      "* Pozosta\u0142e funkcje aktywacji najcze\u015bciej maj\u0105 posta\u0107 sigmoidy (np. funkcja logistyczna lub tangens hiperboliczny, $\\tanh$).\n",
      "* Parametry $\\Theta$ to wagi na po\u0142\u0105czeniach miedzy neuronami dw\u00f3ch warstw. Rozmiar macierzy $\\Theta^{(i)}$, czyli macierzy wag na po\u0142\u0105czeniach warstw $a^{(i-1)}$ i $a^{(i)}$, to $\\dim(a^{(i-1)}) \\times \\dim(a^{(i)})$. \n",
      "* Parametry $\\beta$ zast\u0119puj\u0105 tutaj dodawanie kolumny z jedynkami do naszej macierzy cech. Macierz $\\beta^{(i)}$ ma rozmiar r\u00f3wny liczbie neuron\u00f3w w odpowiedniej warstwie, czyli $1 \\times \\dim(a^{(i)})$.\n",
      "\n",
      "## Zadania (cze\u015b\u0107 teoretyczna)\n",
      "\n",
      "* Rozwin rekurencj\u0119\u00a0dla dwuwarstowej sieci neuronowej (zapisz r\u00f3wnania w postaci Latexowej dla $a^{(0)}$, $a^{(1)}$, $a^{(2)}$) wraz z funkcj\u0105\u00a0decyzyjn\u0105 dla klasyfikacji.\n",
      "* Pomy\u015blmy ponownie o danych MNIST. Za\u0142o\u017cmy, \u017ce mamy dwuwarstwow\u0105 sie\u0107 neuronow\u0105 z warstw\u0105\u00a0ukryt\u0105 o liczbie 100 neuron\u00f3w. Podaj wymiary macierzy  $\\Theta^{(1)}, \\Theta^{(2)} $ oraz $\\beta^{(1)}, \\beta^{(2)} $. Uzasadnij.\n",
      "* Jakie b\u0119d\u0105 wymiary odpowiednich macierzy w przypadku trzywarstowej sieci z ukrytymi warstwami o rozmiarach 200 i 50? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Zadania (cz\u0119\u015b\u0107 praktyczna)\n",
      "\n",
      "### Dwuwarstwowa sie\u0107 neuronowa"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "Theta1 = np.matrix(np.load(\"2layers/Theta1.npy\"))\n",
      "Beta1 = np.matrix(np.load(\"2layers/Beta1.npy\"))\n",
      "\n",
      "Theta2 = np.matrix(np.load(\"2layers/Theta2.npy\"))\n",
      "Beta2 = np.matrix(np.load(\"2layers/Beta2.npy\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Powy\u017cej odczytano z dysku cztery macierze  $\\Theta^{(1)}, \\beta^{(1)} $ oraz $ \\Theta^{(2)}, \\beta^{(2)} $ dla zagadnienia MNIST. Macierze te s\u0105 wynikiem pewnego trenowania i mo\u017cna je ju\u017c wykorzysta\u0107 do klasyfikacji za pomoc\u0105 odpowiedniej sieci dwuwarstwowej. Sprawd\u017a ich wymiary. Jaki jest rozmiar warstwy ukrytej?\n",
      "* Zaimplementuj tak\u0105\u00a0sie\u0107 zgodnie z powy\u017cszymi wzorami i za pomoc\u0105\u00a0podanych macierzy. Jako funkcj\u0119\u00a0aktywacji przyjmij odpowiednio $g^{(1)}(x) = \\tanh(x)$ oraz $g^{(2)}(x) = \\mathop{\\mathrm{softmax}}(x)$.\n",
      "* Zastosuj do zestawu MNIST i podaj poprawno\u015b\u0107 klasyfikacyjn\u0105, kt\u00f3r\u0105 ta sie\u0107 uzyskuje (powinna by\u0107 bardzo wysoka). <br/> **UWAGA**: Ju\u017c nie jest potrzebny *bias term* w naszej macierzy cech, dodawanie parametr\u00f3w $\\beta$ jest odpowiednikiem tej operacji. Wektory cech powinny mie\u0107\u00a0zatem d\u0142ugo\u015b\u0107\u00a0$28 \\times 28$ (same piksele) a nie $28 \\times 28 + 1$ jak do tej pory."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "### 6-ciowarstwowa sie\u0107 neuronowa\n",
      "\n",
      "Analogicznie, w katalogu \"6layers\" mamy sie\u0107 6-ciowarstwow\u0105. \n",
      "* Zaimplementuj sie\u0107. \n",
      "* Wykonaj analogicznie obliczenia jak dla sieci dwuwarstwowej i podaj jako\u015b\u0107 kasyfikacji. Podobnie jak wy\u017cej ostatnia warstwa ma funkcj\u0119 aktywacji $\\mathrm{softmax}$, pozosta\u0142e to $\\mathrm{tanh}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}