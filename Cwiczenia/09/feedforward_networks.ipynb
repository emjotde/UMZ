{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja za pomocą sieci neuronowych \n",
    "\n",
    "## Przypomnienie materiału z wykładu\n",
    "\n",
    "Podczas klasyfikowania danych wejściowych za pomocą prostych jednokierunkowych sieci neuronowych (*ang.* **feedforward**) obliczamy wartości neuronów na podstawie wartości neuronów poprzedniej warstwy.\n",
    "\n",
    "* Mając daną $n$-warstwową sieć neuronową oraz jej parametry $\\Theta^{(1)}, \\ldots, \\Theta^{(n)} $ oraz $\\beta^{(1)}, \\ldots, \\beta^{(n)} $ liczymy: \n",
    "$$a^{(i)} = g^{(i-1)}\\left( a^{(i-1)} \\Theta^{(i)} + \\beta^{(i)} \\right). $$ \n",
    "* Funkcje $g^{(i)}$ to tzw. **funkcje aktywacji**.\n",
    "* Dla $i = 0$ przyjmujemy $a^{(0)} = \\mathrm{x}$ (wektor wierszowy cech) oraz $g^{(0)}(x) = x$ (identyczność).\n",
    "* W przypadku klasyfikacji, często dla ostatniej warstwy $n$ (o rozmiarze równym liczbie klas) przyjmuje się $g^{(n)}(x) = \\mathop{\\mathrm{softmax}}(x)$.\n",
    "* Wtedy funkcja decyzyjna ma postać:\n",
    "$$c = \\mathop{\\mathrm{argmax}}_{i}\\left(a^{(n)}_i\\right)$$\n",
    "* Pozostałe funkcje aktywacji najcześciej mają postać sigmoidy (np. funkcja logistyczna lub tangens hiperboliczny, $\\tanh$).\n",
    "* Parametry $\\Theta$ to wagi na połączeniach miedzy neuronami dwóch warstw. Rozmiar macierzy $\\Theta^{(i)}$, czyli macierzy wag na połączeniach warstw $a^{(i-1)}$ i $a^{(i)}$, to $\\dim(a^{(i-1)}) \\times \\dim(a^{(i)})$. \n",
    "* Parametry $\\beta$ zastępują tutaj dodawanie kolumny z jedynkami do naszej macierzy cech. Macierz $\\beta^{(i)}$ ma rozmiar równy liczbie neuronów w odpowiedniej warstwie, czyli $1 \\times \\dim(a^{(i)})$.\n",
    "\n",
    "## Zadania (cześć teoretyczna)\n",
    "\n",
    "* Rozwin rekurencję dla dwuwarstowej sieci neuronowej (zapisz równania w postaci Latexowej dla $a^{(0)}$, $a^{(1)}$, $a^{(2)}$) wraz z funkcją decyzyjną dla klasyfikacji.\n",
    "* Pomyślmy ponownie o danych MNIST. Założmy, że mamy dwuwarstwową sieć neuronową z warstwą ukrytą o liczbie 100 neuronów. Podaj wymiary macierzy  $\\Theta^{(1)}, \\Theta^{(2)} $ oraz $\\beta^{(1)}, \\beta^{(2)} $. Uzasadnij.\n",
    "* Jakie będą wymiary odpowiednich macierzy w przypadku trzywarstowej sieci z ukrytymi warstwami o rozmiarach 200 i 50? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania (część praktyczna)\n",
    "\n",
    "### Dwuwarstwowa sieć neuronowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Theta1 = np.matrix(np.load(\"2layers/Theta1.npy\"))\n",
    "Beta1 = np.matrix(np.load(\"2layers/Beta1.npy\"))\n",
    "\n",
    "Theta2 = np.matrix(np.load(\"2layers/Theta2.npy\"))\n",
    "Beta2 = np.matrix(np.load(\"2layers/Beta2.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Powyżej odczytano z dysku cztery macierze  $\\Theta^{(1)}, \\beta^{(1)} $ oraz $ \\Theta^{(2)}, \\beta^{(2)} $ dla zagadnienia MNIST. Macierze te są wynikiem pewnego trenowania i można je już wykorzystać do klasyfikacji za pomocą odpowiedniej sieci dwuwarstwowej. Sprawdź ich wymiary. Jaki jest rozmiar warstwy ukrytej?\n",
    "* Zaimplementuj taką sieć zgodnie z powyższymi wzorami i za pomocą podanych macierzy. Jako funkcję aktywacji przyjmij odpowiednio $g^{(1)}(x) = \\tanh(x)$ oraz $g^{(2)}(x) = \\mathop{\\mathrm{softmax}}(x)$.\n",
    "* Zastosuj do zestawu MNIST i podaj poprawność klasyfikacyjną, którą ta sieć uzyskuje (powinna być bardzo wysoka). <br/> **UWAGA**: Już nie jest potrzebny *bias term* w naszej macierzy cech, dodawanie parametrów $\\beta$ jest odpowiednikiem tej operacji. Wektory cech powinny mieć zatem długość $28 \\times 28$ (same piksele) a nie $28 \\times 28 + 1$ jak do tej pory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6-ciowarstwowa sieć neuronowa\n",
    "\n",
    "Analogicznie, w katalogu \"6layers\" mamy sieć 6-ciowarstwową. \n",
    "* Zaimplementuj sieć. \n",
    "* Wykonaj analogicznie obliczenia jak dla sieci dwuwarstwowej i podaj jakość kasyfikacji. Podobnie jak wyżej ostatnia warstwa ma funkcję aktywacji $\\mathrm{softmax}$, pozostałe to $\\mathrm{tanh}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
